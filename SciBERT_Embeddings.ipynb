{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SciBERT Embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ],
      "metadata": {
        "id": "JyINv6S1EmwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "R65ikQSnGRVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqGmZdQ-ENWE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import csv\n",
        "import gzip\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from random import randint\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "import networkx as nx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SciBERT"
      ],
      "metadata": {
        "id": "rISS53HHEfgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "G = nx.read_edgelist('data/edgelist.txt', delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
        "nodes = list(G.nodes())\n",
        "n = G.number_of_nodes()\n",
        "m = G.number_of_edges()\n",
        "print('Number of nodes:', n)\n",
        "print('Number of edges:', m)\n",
        "\n",
        "abstracts = dict()\n",
        "with open('data/abstracts.txt', 'r',  encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        node, abstract = line.split('|--|')\n",
        "        abstracts[int(node)] = abstract"
      ],
      "metadata": {
        "id": "Ty97rJ3PEgYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
        "model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'  \n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "text2vec = dict()\n",
        "\n",
        "for i in tqdm(range(0, len(nodes))):\n",
        "    node_id = nodes[i]\n",
        "    abstract = abstracts[node_id]\n",
        "\n",
        "    tokens = tokenizer.encode(abstract, padding=True, truncation=True, max_length=16, add_special_tokens=True, return_tensors='pt').to(device)\n",
        "    token_embeddings = model(tokens)[0].detach().cpu().numpy()\n",
        "    token_embeddings = token_embeddings.squeeze(0)\n",
        "    text2vec[node_id] = token_embeddings"
      ],
      "metadata": {
        "id": "Xnb_EU7UEp-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = gzip.GzipFile('embeddings/abstract_embeddings.emb', 'wb')\n",
        "file.write(pickle.dumps(text2vec))\n",
        "file.close()"
      ],
      "metadata": {
        "id": "nfz5K9yyIfD9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}